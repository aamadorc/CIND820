{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TX_-QfCapg5r"
      },
      "source": [
        "# Prediction of COVID-19 around the world\n",
        "\n",
        "Student: Angela Amador\n",
        "\n",
        "TMU Student Number: 500259095\n",
        "\n",
        "Supervisor: Tamer Abdou, PhD\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KPUF4R_NlrMT"
      },
      "source": [
        "I aim to demonstrate how Machine Learning (ML) models were able to predict the spread of COVID-19 around the world.\n",
        "\n",
        "First, I will explore the dataset to get insides and better understand patterns, detect error and outliers, and find relationships between variables. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s2h0BVIqmIpE"
      },
      "source": [
        "## Preparation\n",
        "This dataset is taken from Our World in Data website, officially collected by Our World in Data team: https://covid.ourworldindata.org/data/owid-covid-data.csv.\n",
        "\n",
        "This dataset will be synced daily. For more info: https://www.kaggle.com/datasets/caesarmario/our-world-in-data-covid19-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from ydata_profiling import ProfileReport\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from numpy import unique\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load file and explore data\n",
        "\n",
        "The dataset, provided by Our World in Data, provides COVID-19 information collected by Our World in Data available to Kaggle community https://www.kaggle.com/datasets/caesarmario/our-world-in-data-covid19-dataset/download?datasetVersionNumber=418. This dataset is updated daily, for the purpose of this study I am analyzing the data with information up to Oct 7th, 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load file\n",
        "raw_data = pd.read_csv('archive.zip', sep=',')  \n",
        "\n",
        "#Explore data\n",
        "raw_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the data type and metadata of the attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at meta information about data, such as null values\n",
        "raw_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see meta information about numeric data, we can also see if there any extreme values\n",
        "raw_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing data before COVID vaccine availability\n",
        "\n",
        "Multiple vaccinates became available on the second semester of 2020. By December most countries have approved vaccinates for their own country. \n",
        "\n",
        "Vaccinations changed the behaviour of the pandemic then I will remove data before Jan 1st, 2021 to consider data only after vaccines became availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Original dataset:\")\n",
        "print(\"Total number of observations: \", raw_data.shape[0])\n",
        "print(\"Total number of attributes: \", raw_data.shape[1])\n",
        "print(\"Size: \", raw_data.size)\n",
        "\n",
        "\n",
        "post_vaccine_data = raw_data.drop(raw_data[raw_data.date < '2021-01-01'].index)\n",
        "\n",
        "print(\"\\nAfter removing data before vaccinate was available around the world (Jan 1st, 2021):\")\n",
        "print(\"Total number of observations: \", post_vaccine_data.shape[0])\n",
        "print(\"Total number of attributes: \", post_vaccine_data.shape[1])\n",
        "print(\"Size: \", post_vaccine_data.size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting\n",
        "One of the first decisions to make is how to utilize the existing data. One common technique is to split the data into two groups typically referred to as the training and testing sets. The training set is used to develop\n",
        "models and feature sets; it is the substrate for estimating parameters, comparing models, and all of the other activities required to reach a final model. The test set is used only at the conclusion of these activities for estimating a final, unbiased assessment of the model’s performance. It is critical that the test set not be used prior to this point. Looking at the test set results would bias the outcomes since the testing data will have become part of the model development process. Reference: Feature Engineering and Selection A Practical Approach for Predictive Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find out the index of total_cases column\n",
        "c = post_vaccine_data.columns.get_loc('total_cases')\n",
        "\n",
        "# Find out number of columns\n",
        "d = post_vaccine_data.shape[1]\n",
        "\n",
        "# Build feature, target arrays\n",
        "X, y = post_vaccine_data.iloc[:, [i for i in range(d) if i != c]], post_vaccine_data.iloc[:, [c]]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1121218)\n",
        "\n",
        "X_train\n",
        "y_train\n",
        "X_test\n",
        "y_test\n",
        "\n",
        "print(\"\\nX_train dataset:\")\n",
        "print(\"Total number of observations: \", X_train.shape[0])\n",
        "print(\"Total number of attributes: \", X_train.shape[1])\n",
        "print(\"Size: \", X_train.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning\n",
        "\n",
        "### Identify Columns That Contain a Single Value\n",
        "The dataset doesn't have columns with a single value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get number of unique values for each column\n",
        "counts = X_train.nunique()\n",
        "\n",
        "# record columns to delete\n",
        "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
        "print(to_del)\n",
        "\n",
        "# drop useless columns\n",
        "X_train.drop(to_del, axis=1, inplace=True)\n",
        "print(X_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Consider Columns That Have Very Few Values\n",
        "Even though we have 23 columns with less than 1% of unique values, most of them are numerical. In addition, some of the columns are categorical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# record columns to delete, columns with unique values less than 1 percent of rows\n",
        "counts\n",
        "to_del = [i for i,v in enumerate(counts) if (float(v)/X_train.shape[0]*100) < 1]\n",
        "print(to_del)\n",
        "\n",
        "# drop useless columns\n",
        "#X_train.drop(to_del, axis=1, inplace=True)\n",
        "#print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing data columns with too many NaN values\n",
        "\n",
        "We can calculate the ratio of missing values using a simple formula. The formula is the number of missing values in each column divided by the total number of observation. Generally, we can drop variables having a missing value ratio of more than 60% or 70%. For my purpose I am going to use a threashold of 60% missing values and remove those attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining threashold of 60% missing values \n",
        "threashold_NaN = 0.60\n",
        "\n",
        "#Explore data\n",
        "def describe_nan(df):\n",
        "    return pd.DataFrame([(i, df[df[i].isna()].shape[0],df[df[i].isna()].shape[0]/df.shape[0]) for i in df.columns], columns=['column', 'nan_counts', 'nan_rate'])\n",
        "\n",
        "pd.options.display.max_rows = None\n",
        "\n",
        "#icu=raw_data.icu_patients.value_counts(dropna=False)\n",
        "#display (\"NaN entries for the icu_patients column:\", icu[icu.index.isnull()])\n",
        "\n",
        "print(\"Attributes with more than 60 percentage of missing values:\")\n",
        "\n",
        "describe_nan(X_train).sort_values(by=\"nan_rate\", ascending=False).query(\"nan_rate >= %s\"%threashold_NaN)\n",
        "\n",
        "#((raw_data.isnull() | raw_data.isna()).sum() * 100 / raw_data.index.size).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drop_columns_NaN = describe_nan(X_train).sort_values(by=\"nan_rate\", ascending=False).query(\"nan_rate >= %s\"%threashold_NaN)[[\"column\"]]\n",
        "drop_columns_NaN = drop_columns_NaN['column'].to_list() \n",
        "\n",
        "# Removing data columns with too many missing values\n",
        "# drop_columns_NaN\n",
        "X_train_NaN = X_train.drop(drop_columns_NaN, axis=1, inplace=False)\n",
        "\n",
        "print(\"After removing columns with more than 60 percentage of missing values:\\n\")\n",
        "print(\"Total number of observations: \", X_train_NaN.shape[0])\n",
        "print(\"Total number of attributes: \", X_train_NaN.shape[1])\n",
        "print(\"Size: \", X_train_NaN.size)\n",
        "print(\"\\n\")\n",
        "X_train_NaN.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Percentage of NaN values per attribute for the remaining columns:\\n\")\n",
        "describe_nan(X_train_NaN).sort_values(by=\"nan_rate\", ascending=False)\n",
        "\n",
        "# del(dr1_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_NaN.head()\n",
        "y_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Low Variance Filter\n",
        "\n",
        "Another way of measuring how much information a data column has, is to measure its variance. In the limit case where the column cells assume a constant value, the variance would be 0 and the column would be of no help in the discrimination of different groups of data.\n",
        "\n",
        "The Low Variance Filter node calculates each column variance and removes those columns with a variance value below a given threshold. Notice that the variance can only be calculated for numerical columns, i.e. this dimensionality reduction method applies only to numerical columns. Note, too, that the variance value depends on the column numerical range. Therefore data column ranges need to be normalized to make variance values independent from the column domain range.\n",
        "\n",
        "First a Normalizer node normalizes all column ranges to [0, 1]; next, a Low Variance Filter node calculates the columns variance and filters out the columns with a variance lower than a set threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization is just like any other Scikit-learn estimator. The default value for the threshold is always 0. \n",
        "# Also, the estimator only works with numeric data obviously and it will raise an error if there are categorical features present in the dataframe. \n",
        "# That’s why, for now, I will subset the numeric features into another dataframe:\n",
        "\n",
        "vt = VarianceThreshold()\n",
        "\n",
        "#dr2 -> Dimensionality Reduction - 2. Removing low variance filter\n",
        "dr2_data_num = dr1_data.select_dtypes(include=\"number\")\n",
        "#dr2_data_num.shape\n",
        "#dr2_data_num.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Before, I need to take care of missing values encoded as NaN natively by replacing with the mean on reduced dataset \"dr2_data_reduced\"\n",
        "\n",
        "print (\"Before replacing NaN values with the mean:\\n\")\n",
        "print(\"Total number of observations: \", dr2_data_num.shape[0])\n",
        "print(\"Total number of attributes: \", dr2_data_num.shape[1])\n",
        "print(\"Size: \", dr2_data_num.size)\n",
        "print(\"\\n\")\n",
        "dr2_data_num.info()\n",
        "\n",
        "for c in dr2_data_num.columns:\n",
        "    dr2_data_num[c] = dr2_data_num[c].fillna(dr2_data_num[c].mean())\n",
        "\n",
        "print (\"\\nAfter replacing NaN values with the mean:\\n\")\n",
        "print(\"Total number of observations: \", dr2_data_num.shape[0])\n",
        "print(\"Total number of attributes: \", dr2_data_num.shape[1])\n",
        "print(\"Size: \", dr2_data_num.size)\n",
        "print(\"\\n\")\n",
        "dr2_data_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we fit the estimator to data and call its get_support() method. It returns a boolean mask with True values for columns which are not dropped. \n",
        "# We can then use this mask to subset our DataFrame like so\n",
        "\n",
        "_ = vt.fit(dr2_data_num)\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_data_num = dr2_data_num.loc[:, mask]\n",
        "\n",
        "# dr2_data_num.shape\n",
        "\n",
        "# dr2_data_num.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We still have the same number of features. Now, let’s drop features with variances close to 0\n",
        "vt = VarianceThreshold(threshold=1)\n",
        "\n",
        "# Fit\n",
        "_ = vt.fit(dr2_data_num)\n",
        "\n",
        "# # Get the boolean mask\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_data_reduced = dr2_data_num.loc[:, mask]\n",
        "\n",
        "print (\"\\nAfter dropping features with variances close to 0:\\n\")\n",
        "print(\"Total number of observations: \", dr2_data_reduced.shape[0])\n",
        "print(\"Total number of attributes: \", dr2_data_reduced.shape[1])\n",
        "print(\"Size: \", dr2_data_reduced.size)\n",
        "print(\"\\n\")\n",
        "dr2_data_reduced.info()\n",
        "\n",
        "# With a threshold of 1, 3 attributes were removedthreshold\n",
        "# From: (255173, 32)\n",
        "# To: (255173, 29)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The attributes that were dropped are:\n",
        "# - reproduction_rate\n",
        "# - new_people_vaccinated_smoothed_per_hundred\n",
        "# - human_development_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method of normalizing all features by dividing them by their mean\n",
        "\n",
        "normalized_df = dr2_data_num / dr2_data_num.mean()\n",
        "normalized_df.head()\n",
        "\n",
        "print(\"Variance of the normalized dataset:\\n\")\n",
        "normalized_df.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, we can use the estimator with a lower threshold like 0.005\n",
        "vt = VarianceThreshold(threshold=0.005)\n",
        "\n",
        "# Fit\n",
        "_ = vt.fit(normalized_df)\n",
        "\n",
        "# # Get the boolean mask\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_data_final = dr2_data_num.loc[:, mask]\n",
        "\n",
        "dr2_data_final.shape\n",
        "\n",
        "# With a threshold of 0.05, zero attributes were removed threshold\n",
        "# From: (255173, 32)\n",
        "# To: (255173, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dr2_data_reduced.columns.get_loc('total_cases')\n",
        "# dr2_data_reduced.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With method of normalizing no attributes were removed; while with variances close to 0, 3 features were removed.\n",
        "# - reproduction_rate\n",
        "# - new_people_vaccinated_smoothed_per_hundred\n",
        "# - human_development_index\n",
        "\n",
        "# I will check if it is rigth to removed these 3 attributes. I will test this by training two RandomForestRegressor to predict a total_cases: the first one on the reduced dataset (dr2_data_reduced), feature selected dataset\n",
        "# and the second one on the full, numeric-feature only dataset (dr2_data_num).\n",
        "\n",
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Find out the index of total_cases column\n",
        "c = dr2_data_reduced.columns.get_loc('total_cases')\n",
        "\n",
        "# Find out number of columns\n",
        "d = dr2_data_reduced.shape[1]\n",
        "\n",
        "# Build feature, target arrays\n",
        "X, y = dr2_data_reduced.iloc[:, [i for i in range(d) if i != c]], dr2_data_reduced.iloc[:, [c]]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1121218)\n",
        "\n",
        "# Init, fit, score\n",
        "forest = RandomForestRegressor(random_state=1121218)\n",
        "\n",
        "_ = forest.fit(X_train, y_train)\n",
        "\n",
        "# Training Score\n",
        "print(f\"Training Score: {forest.score(X_train, y_train)}\")\n",
        "#Training Score: 0.9999950801210624\n",
        "\n",
        "print(f\"Test Score: {forest.score(X_test, y_test)}\")\n",
        "# Test Score: 0.9999399219055796\n",
        "\n",
        "print(\"Both training and test score suggest a really high performance without overfitting.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dr2_data_num.columns.get_loc('total_cases')\n",
        "# dr2_data_num.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, let’s train the same model on the full numeric-only dataset\n",
        "\n",
        "# Find out the index of total_cases column\n",
        "c = dr2_data_num.columns.get_loc('total_cases')\n",
        "\n",
        "# Find out number of columns\n",
        "d = dr2_data_num.shape[1]\n",
        "\n",
        "\n",
        "# Build feature, target arrays\n",
        "X, y = dr2_data_num.iloc[:, [i for i in range(d) if i != c]], dr2_data_num.iloc[:, [c]]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1121218)\n",
        "\n",
        "# Init, fit, score\n",
        "forest = RandomForestRegressor(random_state=1121218)\n",
        "\n",
        "_ = forest.fit(X_train, y_train)\n",
        "\n",
        "# Training Score\n",
        "print(f\"Training Score: {forest.score(X_train, y_train)}\")\n",
        "#Training Score: 0.9999920034215055\n",
        "\n",
        "print(f\"Test Score: {forest.score(X_test, y_test)}\")\n",
        "# Test Score: 0.9999035126774154\n",
        "\n",
        "print(\"I can confirm that there isn't any impact on the prediction by removing these 3 features\")\n",
        "\n",
        "#Freeing memory\n",
        "del(X)\n",
        "del(y)\n",
        "del(X_train)\n",
        "del(X_test)\n",
        "del(y_train)\n",
        "del(y_test)\n",
        "# del(dr2_data_num)\n",
        "# del(dr2_data_reduced)\n",
        "# del(dr2_data_final)\n",
        "del(normalized_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Droping the columns identified with variance close to 0\n",
        "# - reproduction_rate\n",
        "# - new_people_vaccinated_smoothed_per_hundred\n",
        "# - human_development_index\n",
        "\n",
        "dr2_data = dr1_data.drop(['reproduction_rate', 'new_people_vaccinated_smoothed_per_hundred', 'human_development_index'], axis=1)\n",
        "print(\"After removing columns identified with variance close to 0:\\n\")\n",
        "print(\"Total number of observations: \", dr2_data.shape[0])\n",
        "print(\"Total number of attributes: \", dr2_data.shape[1])\n",
        "print(\"Size: \", dr2_data.size)\n",
        "print(\"\\n\")\n",
        "dr2_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. High correlation with other data columns\n",
        "\n",
        "\n",
        "* https://www.kaggle.com/code/bbloggsbott/feature-selection-correlation-and-p-value\n",
        "\n",
        "### Selecting columns based on correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Dataset copy to be use in the correlation, remove the column total_cases because it is the column we are trying to predict\n",
        "#data = raw_data.drop(['total_cases'], axis=1)\n",
        "dr3_data_corr = dr2_data.copy()\n",
        "\n",
        "# The numpy.random.seed() makes the random numbers predictable and is used for reproducibility\n",
        "np.random.seed(123)\n",
        "\n",
        "# Find out the index for categorical variables\n",
        "continent = dr3_data_corr.columns.get_loc('continent')\n",
        "location = dr3_data_corr.columns.get_loc('location')\n",
        "date = dr3_data_corr.columns.get_loc('date')\n",
        "\n",
        "# Encode the Categorical Variable\n",
        "# The dataset has 3 categorical attributes: date, continent and location\n",
        "label_encoder = LabelEncoder()\n",
        "dr3_data_corr.iloc[:,continent] = label_encoder.fit_transform(dr3_data_corr.iloc[:,continent]).astype('float64')\n",
        "dr3_data_corr.iloc[:,location] = label_encoder.fit_transform(dr3_data_corr.iloc[:,location]).astype('float64')\n",
        "dr3_data_corr.iloc[:,date] = label_encoder.fit_transform(dr3_data_corr.iloc[:,date]).astype('float64')\n",
        "\n",
        "corr = dr3_data_corr.corr()\n",
        "corr\n",
        "\n",
        "sns.heatmap(corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Next, compare the correlation between features and remove one of two features that have a correlation higher than 0.9\n",
        "\n",
        "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
        "for i in range(corr.shape[0]):\n",
        "    for j in range(i+1, corr.shape[0]):\n",
        "        if corr.iloc[i,j] >= 0.9:\n",
        "            if columns[j]:\n",
        "                columns[j] = False\n",
        "\n",
        "selected_columns = dr3_data_corr.columns[columns]\n",
        "# selected_columns\n",
        "# selected_columns.shape\n",
        "\n",
        "## Add total_cases column which is the column to be predicted\n",
        "total_cases = pd.Index(['total_cases'])\n",
        "selected_columns = selected_columns.append(total_cases)\n",
        "selected_columns\n",
        "selected_columns.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr3_data_corr = dr3_data_corr[selected_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print (\"After removing one of two features that have a correlation higher than 0.9:\\n\")\n",
        "print(\"Total number of observations: \", dr3_data_corr.shape[0])\n",
        "print(\"Total number of attributes: \", dr3_data_corr.shape[1])\n",
        "print(\"Size: \", dr3_data_corr.size)\n",
        "print(\"\\n\")\n",
        "dr3_data_corr.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Selecting columns based on p-value\n",
        "\n",
        "Selecting the columns based on how they affect the p-value. \n",
        "\n",
        "Column total_cases because was removed, this is the column to be predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removing the total_cases column\n",
        "selected_columns = selected_columns[0:-1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Take care of missing values encoded as NaN natively by replacing with the mean \n",
        "\n",
        "for c in dr3_data_corr.columns:\n",
        "    dr3_data_corr[c] = dr3_data_corr[c].fillna(dr3_data_corr[c].mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "def backwardElimination(x, Y, sl, columns):\n",
        "    numVars = len(x[0])\n",
        "    #x=np.array(x, dtype=float)\n",
        "    for i in range(0, numVars):\n",
        "        regressor_OLS = sm.OLS(Y, x).fit()\n",
        "        maxVar = max(regressor_OLS.pvalues).astype(float)\n",
        "        if maxVar > sl:\n",
        "            for j in range(0, numVars - i):\n",
        "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
        "                    x = np.delete(x, j, 1)\n",
        "                    columns = np.delete(columns, j)\n",
        "                    \n",
        "    \n",
        "    return x, columns, regressor_OLS.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dr3_data_corr.head()\n",
        "# dr3_data_corr.iloc[:,:-1].values \n",
        "# dr3_data_corr.iloc[:,-1].values\n",
        "# selected_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SL = 0.05\n",
        "data_modeled, selected_columns, summary = backwardElimination(\n",
        "    dr3_data_corr.iloc[:,:-1].values ,\n",
        "    dr3_data_corr.iloc[:,-1].values,\n",
        "    SL,\n",
        "    selected_columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_modeled\n",
        "# selected_columns\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = pd.DataFrame()\n",
        "result['total_cases'] = dr3_data_corr.iloc[:,-1]\n",
        "\n",
        "dr3_data = pd.DataFrame(data = data_modeled, columns = selected_columns)\n",
        "\n",
        "# dr3_data['total_cases'] = result['total_cases'] \n",
        "\n",
        "dr3_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "fig = plt.figure(figsize = (20, 25))\n",
        "j = 0\n",
        "for i in dr3_data.columns:\n",
        "    plt.subplot(7, 4, j+1)\n",
        "    j += 1\n",
        "    sns.distplot(dr3_data[i])\n",
        "    #plt.legend(loc='best',fontsize=10)\n",
        "fig.suptitle('Covid Total Cases Data Analysis')\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=0.95)\n",
        "\n",
        "\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Profiling Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the final dataset with the selected columns\n",
        "\n",
        "selected_columns = np.append(selected_columns, 'total_cases')\n",
        "selected_columns\n",
        "selected_columns.shape\n",
        "\n",
        "dr3_data_final = pd.DataFrame(data = dr2_data, columns = selected_columns)\n",
        "\n",
        "print (\"Final dataset:\\n\")\n",
        "print(\"Total number of observations: \", dr3_data_final.shape[0])\n",
        "print(\"Total number of attributes: \", dr3_data_final.shape[1])\n",
        "print(\"Size: \", dr3_data_final.size)\n",
        "print(\"\\n\")\n",
        "dr3_data_final.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Genetate profiling report\n",
        "#profile = ProfileReport(dr3_data_final, title=\"Profiling Report\")\n",
        "profile = ProfileReport(dr3_data_final, title=\"Profiling Report\", html={'style':{'fullwith':True}})\n",
        "profile"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
