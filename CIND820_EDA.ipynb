{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TX_-QfCapg5r"
      },
      "source": [
        "# CIND820 - Exploration Data Analysis  \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KPUF4R_NlrMT"
      },
      "source": [
        "I aim to predict the efficiency of COVID-19 vaccinates around the world using Data Classification and Clustering to analyze the efficiency of COVID-19 vaccines over the population infected and deaths reported.\n",
        "\n",
        "First, I will explore the dataset to get insides and better understand patterns, detect error and outliers, and find relationships between variables. Then, identify key factors to determine the efficiency of COVID-19 vaccine in relation to the number of cases and deaths.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s2h0BVIqmIpE"
      },
      "source": [
        "# Preparation\n",
        "Describing the working dataset and any imposed constraints\n",
        "\n",
        "This dataset is taken from Our World in Data website, officially collected by Our World in Data team. This dataset will be synced daily. For more info:\n",
        "https://www.kaggle.com/datasets/caesarmario/our-world-in-data-covid19-dataset\n",
        "\n",
        "Import the following files:\n",
        "This dataset is taken from Our World in Data website, officially collected by Our World in Data team. This dataset will be synced daily:\n",
        "\n",
        "https://covid.ourworldindata.org/data/owid-covid-data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load file and explore data\n",
        "\n",
        "The dataset, provided by Our World in Data, provides COVID-19 vaccination information collected by Our World in Data available to Kaggle community https://www.kaggle.com/datasets/caesarmario/our-world-in-data-covid19-dataset/download?datasetVersionNumber=418. This dataset is updated daily, for the purpose of this study I am analyzing the data with information up to Oct 7th, 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load file\n",
        "covid_data = pd.read_csv('archive.zip', sep=',')  \n",
        "\n",
        "#Explore data\n",
        "covid_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the data type and metadata of the attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "covid_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at meta information about data, such as null values\n",
        "covid_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see meta information about numeric data, we can also see if there any extreme values\n",
        "covid_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing data before COVID vaccinate availability\n",
        "\n",
        "Multiple vaccinates became available on the second semester of 2020. By December most contries have approved vaccinates for their own country. \n",
        "\n",
        "To avoid ..... we will remove data before Jan 1st, 2021 to consider data only with vaccinate availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "covid_data = covid_data.drop(covid_data[covid_data.date < '2021-01-01'].index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at meta information about data, such as null values\n",
        "covid_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensional Reduction (CMTH642 - Module 9)\n",
        "\n",
        "Due to the size of the dataset with 255173 entries and 67 columns, I am going to apply dimensional reduction to provide better features for statistical learning methods\n",
        "\n",
        "## 1. Removing data columns with too many NaN values\n",
        "\n",
        "We can calculate the ratio of missing values using a simple formula. The formula is- the number of missing values in each column divided by the total number of observation. Generally, we can drop variables having a missing value ratio of more than 60% or 70%. For my purpose I am going to use a threashold of 60% missing values and remove those attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining threashold of 60% missing values \n",
        "threashold_NaN = 0.60\n",
        "\n",
        "#Explore data\n",
        "def describe_nan(df):\n",
        "    return pd.DataFrame([(i, df[df[i].isna()].shape[0],df[df[i].isna()].shape[0]/df.shape[0]) for i in df.columns], columns=['column', 'nan_counts', 'nan_rate'])\n",
        "\n",
        "pd.options.display.max_rows = None\n",
        "\n",
        "#icu=covid_data.icu_patients.value_counts(dropna=False)\n",
        "#display (\"NaN entries for the icu_patients column:\", icu[icu.index.isnull()])\n",
        "\n",
        "describe_nan(covid_data).sort_values(by=\"nan_rate\", ascending=False).query(\"nan_rate >= %s\"%threashold_NaN)\n",
        "\n",
        "#((covid_data.isnull() | covid_data.isna()).sum() * 100 / covid_data.index.size).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "my_columns = describe_nan(covid_data).sort_values(by=\"nan_rate\", ascending=False).query(\"nan_rate < %s\"%threashold_NaN)[[\"column\"]]\n",
        "my_columns = my_columns['column'].to_list() \n",
        "\n",
        "#dr1 -> Dimensionality Reduction - 1. Removing data columns with too many missing values\n",
        "dr1_covid_data = covid_data[my_columns]\n",
        "dr1_covid_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#covid_data.info()\n",
        "describe_nan(dr1_covid_data).sort_values(by=\"nan_rate\", ascending=False)\n",
        "dr1_covid_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "covid_data.size\n",
        "dr1_covid_data.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr1_covid_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Low Variance Filter\n",
        "\n",
        "Another way of measuring how much information a data column has, is to measure its variance. In the limit case where the column cells assume a constant value, the variance would be 0 and the column would be of no help in the discrimination of different groups of data.\n",
        "\n",
        "The Low Variance Filter node calculates each column variance and removes those columns with a variance value below a given threshold. Notice that the variance can only be calculated for numerical columns, i.e. this dimensionality reduction method applies only to numerical columns. Note, too, that the variance value depends on the column numerical range. Therefore data column ranges need to be normalized to make variance values independent from the column domain range.\n",
        "\n",
        "First a Normalizer node normalizes all column ranges to [0, 1]; next, a Low Variance Filter node calculates the columns variance and filters out the columns with a variance lower than a set threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We initialize it just like any other Scikit-learn estimator. The default value for the threshold is always 0. \n",
        "# Also, the estimator only works with numeric data obviously and it will raise an error if there are categorical features present in the dataframe. \n",
        "# That’s why, for now, we will subset the numeric features into another dataframe:\n",
        "vt = VarianceThreshold()\n",
        "\n",
        "#dr2 -> Dimensionality Reduction - 1. Removing low variance filter\n",
        "dr2_covid_data_num = dr1_covid_data.select_dtypes(include=\"number\")\n",
        "dr2_covid_data_num.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Before, I need to tak care of missing values encoded as NaN natively by replacing with the mean on reduced dataset \"dr2_covid_data_reduced\"\n",
        "\n",
        "for c in dr2_covid_data_num.columns:\n",
        "    dr2_covid_data_num[c] = dr2_covid_data_num[c].fillna(dr2_covid_data_num[c].mean())\n",
        "\n",
        "dr2_covid_data_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we fit the estimator to data and call its get_support() method. It returns a boolean mask with True values for columns which are not dropped. \n",
        "# We can then use this mask to subset our DataFrame like so\n",
        "\n",
        "_ = vt.fit(dr2_covid_data_num)\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_covid_data_num = dr2_covid_data_num.loc[:, mask]\n",
        "\n",
        "dr2_covid_data_num.shape\n",
        "\n",
        "dr2_covid_data_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We still have the same number of features. Now, let’s drop features with variances close to 0\n",
        "vt = VarianceThreshold(threshold=1)\n",
        "\n",
        "# Fit\n",
        "_ = vt.fit(dr2_covid_data_num)\n",
        "\n",
        "# # Get the boolean mask\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_covid_data_reduced = dr2_covid_data_num.loc[:, mask]\n",
        "\n",
        "dr2_covid_data_reduced.shape\n",
        "\n",
        "# With a threshold of 1, 3 attributes were removedthreshold\n",
        "# From: (255173, 32)\n",
        "# To: (255173, 29)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr2_covid_data_reduced.info()\n",
        "\n",
        "# The attributes that were dropped are:\n",
        "# - reproduction_rate\n",
        "# - new_people_vaccinated_smoothed_per_hundred\n",
        "# - human_development_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "covid_data.size\n",
        "dr1_covid_data.size\n",
        "dr2_covid_data_reduced.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method of normalizing all features by dividing them by their mean\n",
        "\n",
        "normalized_df = dr2_covid_data_num / dr2_covid_data_num.mean()\n",
        "normalized_df.head()\n",
        "normalized_df.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, we can use the estimator with a lower threshold like 0.005\n",
        "vt = VarianceThreshold(threshold=0.005)\n",
        "\n",
        "# Fit\n",
        "_ = vt.fit(normalized_df)\n",
        "\n",
        "# # Get the boolean mask\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_covid_data_final = dr2_covid_data_num.loc[:, mask]\n",
        "\n",
        "dr2_covid_data_final.shape\n",
        "\n",
        "# With a threshold of 0.05, zero attributes were removedthreshold\n",
        "# From: (255173, 32)\n",
        "# To: (255173, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr2_covid_data_reduced.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With method of normalizing no attributes were removed; while with variances close to 0, 3 features were removed.\n",
        "\n",
        "# I will check is it is rigth to removed this 3 attributes. I will test this by training two RandomForestRegressor to predict a total_cases: the first one on the reduced dataset, feature selected dataset\n",
        "# and the second one on the full, numeric-feature only dataset.\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Build feature, target arrays\n",
        "X, y = dr2_covid_data_reduced.iloc[:, [i for i in range(29) if i != 18]], dr2_covid_data_reduced.iloc[:, [18]]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1121218)\n",
        "\n",
        "# Init, fit, score\n",
        "forest = RandomForestRegressor(random_state=1121218)\n",
        "\n",
        "_ = forest.fit(X_train, y_train)\n",
        "\n",
        "# Training Score\n",
        "print(f\"Training Score: {forest.score(X_train, y_train)}\")\n",
        "#Training Score: 0.988528867222243\n",
        "\n",
        "print(f\"Test Score: {forest.score(X_test, y_test)}\")\n",
        "# Test Score: 0.9511616691995844"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dr2_covid_data_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Both training and test score suggest a really high performance without overfitting. Now, let’s train the same model on the full numeric-only dataset\n",
        "\n",
        "# Build feature, target arrays\n",
        "X, y = dr2_covid_data_num.iloc[:, [i for i in range(32) if i != 21]], dr2_covid_data_num.iloc[:, [21]]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1121218)\n",
        "\n",
        "# Init, fit, score\n",
        "forest = RandomForestRegressor(random_state=1121218)\n",
        "\n",
        "_ = forest.fit(X_train, y_train)\n",
        "\n",
        "# Training Score\n",
        "print(f\"Training Score: {forest.score(X_train, y_train)}\")\n",
        "#Training Score: 0.988528867222243\n",
        "\n",
        "print(f\"Test Score: {forest.score(X_test, y_test)}\")\n",
        "\n",
        "#I can confirm that there isn't any impact on the prediction by removing these 3 features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Genetate profiling report\n",
        "#profile = ProfileReport(covid_data, title=\"Profiling Report\")\n",
        "#profile = ProfileReport(covid_data, title=\"Profiling Report\", html={'style':{'fullwith':True}})\n",
        "#profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. High correlation with other data columns\n",
        "\n",
        "\n",
        "* https://www.kaggle.com/code/bbloggsbott/feature-selection-correlation-and-p-value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "corr = dr2_covid_data_final.corr()\n",
        "corr.head()\n",
        "\n",
        "sns.heatmap(corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns = np.full((corr.shape[0],), True, dtype=bool)\n",
        "for i in range(corr.shape[0]):\n",
        "    for j in range(i+1, corr.shape[0]):\n",
        "        if corr.iloc[i,j] >= 0.9:\n",
        "            if columns[j]:\n",
        "                columns[j] = False\n",
        "\n",
        "selected_columns = dr2_covid_data_final.columns[columns]\n",
        "selected_columns\n",
        "selected_columns.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* https://towardsdatascience.com/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
