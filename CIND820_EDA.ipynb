{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TX_-QfCapg5r"
      },
      "source": [
        "# Prediction of COVID-19 around the world\n",
        "\n",
        "Student: Angela Amador\n",
        "\n",
        "TMU Student Number: 500259095\n",
        "\n",
        "Supervisor: Tamer Abdou, PhD\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KPUF4R_NlrMT"
      },
      "source": [
        "I aim to demonstrate how Machine Learning (ML) models were able to predict the spread of COVID-19 around the world.\n",
        "\n",
        "First, I will explore the dataset to get insides and better understand patterns, detect error and outliers, and find relationships between variables. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s2h0BVIqmIpE"
      },
      "source": [
        "## Preparation\n",
        "Describing the working dataset and any imposed constraints\n",
        "\n",
        "This dataset is taken from Our World in Data website, officially collected by Our World in Data team: https://covid.ourworldindata.org/data/owid-covid-data.csv.\n",
        "\n",
        "This dataset will be synced daily. For more info: https://www.kaggle.com/datasets/caesarmario/our-world-in-data-covid19-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from ydata_profiling import ProfileReport\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load file and explore data\n",
        "\n",
        "The dataset, provided by Our World in Data, provides COVID-19 information collected by Our World in Data available to Kaggle community https://www.kaggle.com/datasets/caesarmario/our-world-in-data-covid19-dataset/download?datasetVersionNumber=418. This dataset is updated daily, for the purpose of this study I am analyzing the data with information up to Oct 7th, 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load file\n",
        "covid_data = pd.read_csv('archive.zip', sep=',')  \n",
        "\n",
        "#Explore data\n",
        "covid_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the data type and metadata of the attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "covid_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at meta information about data, such as null values\n",
        "covid_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see meta information about numeric data, we can also see if there any extreme values\n",
        "covid_data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Removing data before COVID vaccinate availability\n",
        "\n",
        "Multiple vaccinates became available on the second semester of 2020. By December most contries have approved vaccinates for their own country. \n",
        "\n",
        "To avoid ..xxxxxx... we will remove data before Jan 1st, 2021 to consider data only with vaccinate availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Original dataset:\")\n",
        "print(\"Total number of observations: \", covid_data.shape[0])\n",
        "print(\"Total number of attributes: \", covid_data.shape[1])\n",
        "print(\"Size: \", covid_data.size)\n",
        "\n",
        "\n",
        "covid_data = covid_data.drop(covid_data[covid_data.date < '2021-01-01'].index)\n",
        "\n",
        "print(\"\\nAfter removing data before vaccinate was available around the world (Jan 1st, 2021):\")\n",
        "print(\"Total number of observations: \", covid_data.shape[0])\n",
        "print(\"Total number of attributes: \", covid_data.shape[1])\n",
        "print(\"Size: \", covid_data.size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical attributes\n",
        "\n",
        "Base of the analysis of the attributes iso_code and location, I can tell that one can be derive from the other. For the purpose of this study, I am going to keep location and remove iso_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# with open(\"data.pickle\", \"wb\") as output:\n",
        "#     pickle.dump(covid_data, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# with open(\"data.pickle\", \"rb\") as input:\n",
        "#     data = pickle.load(input)\n",
        "\n",
        "covid_data.groupby([\"iso_code\"])[\"iso_code\"].count()\n",
        "covid_data.groupby([\"location\"])[\"location\"].count()\n",
        "\n",
        "covid_data = covid_data.drop(['iso_code'], axis=1)\n",
        "\n",
        "print(\"\\nAfter removing attribute iso_code because it can be delivered from location:\")\n",
        "print(\"Total number of observations: \", covid_data.shape[0])\n",
        "print(\"Total number of attributes: \", covid_data.shape[1])\n",
        "print(\"Size: \", covid_data.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dimensional Reduction (CMTH642 - Module 9)\n",
        "\n",
        "Due to the size of the dataset with 255,173 entries and 67 columns, I am going to apply dimensional reduction to provideÂ better features for statistical learning methods\n",
        "\n",
        "## 1. Removing data columns with too many NaN values\n",
        "\n",
        "We can calculate the ratio of missing values using a simple formula. The formula is- the number of missing values in each column divided by the total number of observation. Generally, we can drop variables having a missing value ratio of more than 60% or 70%. For my purpose I am going to use a threashold of 60% missing values and remove those attributes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining threashold of 60% missing values \n",
        "threashold_NaN = 0.60\n",
        "\n",
        "#Explore data\n",
        "def describe_nan(df):\n",
        "    return pd.DataFrame([(i, df[df[i].isna()].shape[0],df[df[i].isna()].shape[0]/df.shape[0]) for i in df.columns], columns=['column', 'nan_counts', 'nan_rate'])\n",
        "\n",
        "pd.options.display.max_rows = None\n",
        "\n",
        "#icu=covid_data.icu_patients.value_counts(dropna=False)\n",
        "#display (\"NaN entries for the icu_patients column:\", icu[icu.index.isnull()])\n",
        "\n",
        "print(\"Attributes with more than 60 percentage of missing values:\")\n",
        "\n",
        "describe_nan(covid_data).sort_values(by=\"nan_rate\", ascending=False).query(\"nan_rate >= %s\"%threashold_NaN)\n",
        "\n",
        "#((covid_data.isnull() | covid_data.isna()).sum() * 100 / covid_data.index.size).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "my_columns = describe_nan(covid_data).sort_values(by=\"nan_rate\", ascending=False).query(\"nan_rate < %s\"%threashold_NaN)[[\"column\"]]\n",
        "my_columns = my_columns['column'].to_list() \n",
        "\n",
        "#dr1 -> Dimensionality Reduction - 1. Removing data columns with too many missing values\n",
        "dr1_covid_data = covid_data[my_columns]\n",
        "\n",
        "print(\"After removing columns with more than 60 percentage of missing values:\\n\")\n",
        "print(\"Total number of observations: \", dr1_covid_data.shape[0])\n",
        "print(\"Total number of attributes: \", dr1_covid_data.shape[1])\n",
        "print(\"Size: \", dr1_covid_data.size)\n",
        "print(\"\\n\")\n",
        "dr1_covid_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Percentage of NaN values per attribute for the remaining columns:\\n\")\n",
        "describe_nan(dr1_covid_data).sort_values(by=\"nan_rate\", ascending=False)\n",
        "\n",
        "# To manage memory dur to the size of the dataset, I am keeping one version of the dataset and removing any temporary copy\n",
        "covid_data = dr1_covid_data\n",
        "del(dr1_covid_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Low Variance Filter\n",
        "\n",
        "Another way of measuring how much information a data column has, is to measure its variance. In the limit case where the column cells assume a constant value, the variance would be 0 and the column would be of no help in the discrimination of different groups of data.\n",
        "\n",
        "The Low Variance Filter node calculates each column variance and removes those columns with a variance value below a given threshold. Notice that the variance can only be calculated for numerical columns, i.e. this dimensionality reduction method applies only to numerical columns. Note, too, that the variance value depends on the column numerical range. Therefore data column ranges need to be normalized to make variance values independent from the column domain range.\n",
        "\n",
        "First a Normalizer node normalizes all column ranges to [0, 1]; next, a Low Variance Filter node calculates the columns variance and filters out the columns with a variance lower than a set threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialization is just like any other Scikit-learn estimator. The default value for the threshold is always 0. \n",
        "# Also, the estimator only works with numeric data obviously and it will raise an error if there are categorical features present in the dataframe. \n",
        "# Thatâs why, for now, I will subset the numeric features into another dataframe:\n",
        "\n",
        "vt = VarianceThreshold()\n",
        "\n",
        "#dr2 -> Dimensionality Reduction - 2. Removing low variance filter\n",
        "dr2_covid_data_num = covid_data.select_dtypes(include=\"number\")\n",
        "#dr2_covid_data_num.shape\n",
        "#dr2_covid_data_num.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Before, I need to take care of missing values encoded as NaN natively by replacing with the mean on reduced dataset \"dr2_covid_data_reduced\"\n",
        "\n",
        "print (\"Before replacing NaN values with the mean:\\n\")\n",
        "print(\"Total number of observations: \", dr2_covid_data_num.shape[0])\n",
        "print(\"Total number of attributes: \", dr2_covid_data_num.shape[1])\n",
        "print(\"Size: \", dr2_covid_data_num.size)\n",
        "print(\"\\n\")\n",
        "dr2_covid_data_num.info()\n",
        "\n",
        "for c in dr2_covid_data_num.columns:\n",
        "    dr2_covid_data_num[c] = dr2_covid_data_num[c].fillna(dr2_covid_data_num[c].mean())\n",
        "\n",
        "print (\"\\nAfter replacing NaN values with the mean:\\n\")\n",
        "print(\"Total number of observations: \", dr2_covid_data_num.shape[0])\n",
        "print(\"Total number of attributes: \", dr2_covid_data_num.shape[1])\n",
        "print(\"Size: \", dr2_covid_data_num.size)\n",
        "print(\"\\n\")\n",
        "dr2_covid_data_num.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, we fit the estimator to data and call its get_support() method. It returns a boolean mask with True values for columns which are not dropped. \n",
        "# We can then use this mask to subset our DataFrame like so\n",
        "\n",
        "_ = vt.fit(dr2_covid_data_num)\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_covid_data_num = dr2_covid_data_num.loc[:, mask]\n",
        "\n",
        "# dr2_covid_data_num.shape\n",
        "\n",
        "# dr2_covid_data_num.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We still have the same number of features. Now, letâs drop features with variances close to 0\n",
        "vt = VarianceThreshold(threshold=1)\n",
        "\n",
        "# Fit\n",
        "_ = vt.fit(dr2_covid_data_num)\n",
        "\n",
        "# # Get the boolean mask\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_covid_data_reduced = dr2_covid_data_num.loc[:, mask]\n",
        "\n",
        "print (\"\\nAfter dropping features with variances close to 0:\\n\")\n",
        "print(\"Total number of observations: \", dr2_covid_data_reduced.shape[0])\n",
        "print(\"Total number of attributes: \", dr2_covid_data_reduced.shape[1])\n",
        "print(\"Size: \", dr2_covid_data_reduced.size)\n",
        "print(\"\\n\")\n",
        "dr2_covid_data_reduced.info()\n",
        "\n",
        "# With a threshold of 1, 3 attributes were removedthreshold\n",
        "# From: (255173, 32)\n",
        "# To: (255173, 29)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The attributes that were dropped are:\n",
        "# - reproduction_rate\n",
        "# - new_people_vaccinated_smoothed_per_hundred\n",
        "# - human_development_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method of normalizing all features by dividing them by their mean\n",
        "\n",
        "normalized_df = dr2_covid_data_num / dr2_covid_data_num.mean()\n",
        "normalized_df.head()\n",
        "\n",
        "print(\"Variance of the normalized dataset:\\n\")\n",
        "normalized_df.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, we can use the estimator with a lower threshold like 0.005\n",
        "vt = VarianceThreshold(threshold=0.005)\n",
        "\n",
        "# Fit\n",
        "_ = vt.fit(normalized_df)\n",
        "\n",
        "# # Get the boolean mask\n",
        "mask = vt.get_support()\n",
        "\n",
        "dr2_covid_data_final = dr2_covid_data_num.loc[:, mask]\n",
        "\n",
        "dr2_covid_data_final.shape\n",
        "\n",
        "# With a threshold of 0.05, zero attributes were removed threshold\n",
        "# From: (255173, 32)\n",
        "# To: (255173, 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dr2_covid_data_reduced.columns.get_loc('total_cases')\n",
        "# dr2_covid_data_reduced.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With method of normalizing no attributes were removed; while with variances close to 0, 3 features were removed.\n",
        "# - reproduction_rate\n",
        "# - new_people_vaccinated_smoothed_per_hundred\n",
        "# - human_development_index\n",
        "\n",
        "# I will check if it is rigth to removed these 3 attributes. I will test this by training two RandomForestRegressor to predict a total_cases: the first one on the reduced dataset (dr2_covid_data_reduced), feature selected dataset\n",
        "# and the second one on the full, numeric-feature only dataset (dr2_covid_data_num).\n",
        "\n",
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Find out the index of total_cases column\n",
        "c = dr2_covid_data_reduced.columns.get_loc('total_cases')\n",
        "\n",
        "# Find out number of columns\n",
        "d = dr2_covid_data_reduced.shape[1]\n",
        "\n",
        "# Build feature, target arrays\n",
        "X, y = dr2_covid_data_reduced.iloc[:, [i for i in range(d) if i != c]], dr2_covid_data_reduced.iloc[:, [c]]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1121218)\n",
        "\n",
        "# Init, fit, score\n",
        "forest = RandomForestRegressor(random_state=1121218)\n",
        "\n",
        "_ = forest.fit(X_train, y_train)\n",
        "\n",
        "# Training Score\n",
        "print(f\"Training Score: {forest.score(X_train, y_train)}\")\n",
        "#Training Score: 0.988528867222243\n",
        "\n",
        "print(f\"Test Score: {forest.score(X_test, y_test)}\")\n",
        "# Test Score: 0.9511616691995844\n",
        "\n",
        "print(\"Both training and test score suggest a really high performance without overfitting.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dr2_covid_data_num.columns.get_loc('total_cases')\n",
        "# dr2_covid_data_num.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now, letâs train the same model on the full numeric-only dataset\n",
        "\n",
        "# Find out the index of total_cases column\n",
        "c = dr2_covid_data_num.columns.get_loc('total_cases')\n",
        "\n",
        "# Find out number of columns\n",
        "d = dr2_covid_data_num.shape[1]\n",
        "\n",
        "\n",
        "# Build feature, target arrays\n",
        "X, y = dr2_covid_data_num.iloc[:, [i for i in range(d) if i != c]], dr2_covid_data_num.iloc[:, [c]]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=1121218)\n",
        "\n",
        "# Init, fit, score\n",
        "forest = RandomForestRegressor(random_state=1121218)\n",
        "\n",
        "_ = forest.fit(X_train, y_train)\n",
        "\n",
        "# Training Score\n",
        "print(f\"Training Score: {forest.score(X_train, y_train)}\")\n",
        "#Training Score: 0.988528867222243\n",
        "\n",
        "print(f\"Test Score: {forest.score(X_test, y_test)}\")\n",
        "\n",
        "print(\"I can confirm that there isn't any impact on the prediction by removing these 3 features\")\n",
        "\n",
        "#Freeing memory\n",
        "del(X)\n",
        "del(y)\n",
        "del(X_train)\n",
        "del(X_test)\n",
        "del(y_train)\n",
        "del(y_test)\n",
        "del(dr2_covid_data_num)\n",
        "del(dr2_covid_data_reduced)\n",
        "del(dr2_covid_data_final)\n",
        "del(normalized_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Droping the columns identified with variance close to 0\n",
        "# - reproduction_rate\n",
        "# - new_people_vaccinated_smoothed_per_hundred\n",
        "# - human_development_index\n",
        "\n",
        "covid_data = covid_data.drop(['reproduction_rate'], axis=1)\n",
        "covid_data = covid_data.drop(['new_people_vaccinated_smoothed_per_hundred'], axis=1)\n",
        "covid_data = covid_data.drop(['human_development_index'], axis=1)\n",
        "print(\"After removing columns identified with variance close to 0:\\n\")\n",
        "print(\"Total number of observations: \", covid_data.shape[0])\n",
        "print(\"Total number of attributes: \", covid_data.shape[1])\n",
        "print(\"Size: \", covid_data.size)\n",
        "print(\"\\n\")\n",
        "covid_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. High correlation with other data columns\n",
        "\n",
        "\n",
        "* https://www.kaggle.com/code/bbloggsbott/feature-selection-correlation-and-p-value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \n",
        "\n",
        "#import seaborn as sns\n",
        "#import matplotlib.pyplot as plt\n",
        "#from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#from sklearn.svm import SVC\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# corr = dr2_covid_data_final.corr()\n",
        "# corr.head()\n",
        "\n",
        "# sns.heatmap(corr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# columns = np.full((corr.shape[0],), True, dtype=bool)\n",
        "# for i in range(corr.shape[0]):\n",
        "#     for j in range(i+1, corr.shape[0]):\n",
        "#         if corr.iloc[i,j] >= 0.9:\n",
        "#             if columns[j]:\n",
        "#                 columns[j] = False\n",
        "\n",
        "# selected_columns = dr2_covid_data_final.columns[columns]\n",
        "# selected_columns\n",
        "# selected_columns.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* https://towardsdatascience.com/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Profiling Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Genetate profiling report\n",
        "#profile = ProfileReport(covid_data, title=\"Profiling Report\")\n",
        "#profile = ProfileReport(covid_data, title=\"Profiling Report\", html={'style':{'fullwith':True}})\n",
        "#profile"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
